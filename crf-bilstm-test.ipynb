{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import operator\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from plot_keras_history import plot_history\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "# from keras_contrib.utils import save_load_utils\n",
    "\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.models import Model\n",
    "# from tensorflow.keras import Input\n",
    "\n",
    "from keras import Input\n",
    "\n",
    "# from keras_contrib.layers import CRF\n",
    "from tensorflow_addons.layers import CRF\n",
    "from keras_contrib import losses\n",
    "from keras_contrib import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtensorflow\u001b[49m\u001b[38;5;241m.\u001b[39mversion)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensorflow' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_gk_train = pd.read_csv('ar_gk_train_cleaned.csv')\n",
    "\n",
    "sents_train = []\n",
    "temp = []\n",
    "tags_train = []\n",
    "temp_tags = []\n",
    "current_id = 0\n",
    "\n",
    "for item in df_ar_gk_train.itertuples(index=False):\n",
    "    if item.sent_id != current_id:\n",
    "        current_id += 1\n",
    "        sents_train.append(temp)\n",
    "        tags_train.append(temp_tags)\n",
    "        temp = []\n",
    "        temp_tags = []\n",
    "    temp.append(item)\n",
    "    temp_tags.append(item.NER_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar_gk_test = pd.read_csv('ar_gk_test_cleaned.csv')\n",
    "\n",
    "sents_test = []\n",
    "temp = []\n",
    "tags_test = []\n",
    "temp_tags = []\n",
    "current_id = 0\n",
    "\n",
    "for item in df_ar_gk_test.itertuples(index=False):\n",
    "    if item.sent_id != current_id:\n",
    "        current_id += 1\n",
    "        sents_test.append(temp)\n",
    "        tags_test.append(temp_tags)\n",
    "        temp = []\n",
    "        temp_tags = []\n",
    "    temp.append(item)\n",
    "    temp_tags.append(item.NER_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents = sents_train + sents_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents_filtered = []\n",
    "\n",
    "start = False\n",
    "\n",
    "for sent in all_sents:\n",
    "    temp = []\n",
    "    for token in sent:\n",
    "        if token.token_text == '-LRB-':\n",
    "            start = True\n",
    "            continue\n",
    "        elif token.token_text == '-RRB-':\n",
    "            start = False\n",
    "            continue\n",
    "        elif not start:\n",
    "            temp.append(token)\n",
    "    all_sents_filtered.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train = []\n",
    "filtered_test = []\n",
    "\n",
    "start = False\n",
    "\n",
    "for sent in sents_train:\n",
    "    temp = []\n",
    "    for token in sent:\n",
    "        if token.token_text == '-LRB-':\n",
    "            start = True\n",
    "            continue\n",
    "        elif token.token_text == '-RRB-':\n",
    "            start = False\n",
    "            continue\n",
    "        elif not start:\n",
    "            temp.append(token)\n",
    "    filtered_train.append(temp)\n",
    "            \n",
    "start = False\n",
    "            \n",
    "for sent in sents_test:\n",
    "    temp = []\n",
    "    for token in sent:\n",
    "        if token.token_text == '-LRB-':\n",
    "            start = True\n",
    "            continue\n",
    "        elif token.token_text == '-RRB-':\n",
    "            start = False\n",
    "            continue\n",
    "        elif not start:\n",
    "            temp.append(token)\n",
    "    filtered_test.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pandas(sent_id=0, token_text='1/2', token_lemma='1/2', token_tag='CD', NER_tags='B-QUANTITY'),\n",
       " Pandas(sent_id=0, token_text='large', token_lemma='large', token_tag='JJ', NER_tags='SIZE'),\n",
       " Pandas(sent_id=0, token_text='sweet', token_lemma='sweet', token_tag='JJ', NER_tags='B-NAME'),\n",
       " Pandas(sent_id=0, token_text='red', token_lemma='red', token_tag='JJ', NER_tags='I-NAME'),\n",
       " Pandas(sent_id=0, token_text='onion', token_lemma='onion', token_tag='NN', NER_tags='I-NAME'),\n",
       " Pandas(sent_id=0, token_text=',', token_lemma=',', token_tag=',', NER_tags='O'),\n",
       " Pandas(sent_id=0, token_text='thinly', token_lemma='thinly', token_tag='RB', NER_tags='O'),\n",
       " Pandas(sent_id=0, token_text='sliced', token_lemma='sliced', token_tag='VBD', NER_tags='B-STATE')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_filtered[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filtered = filtered_test + filtered_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Pandas(sent_id=5, token_text='3/4', token_lemma='3/4', token_tag='CD', NER_tags='B-QUANTITY'),\n",
       " Pandas(sent_id=5, token_text='cup', token_lemma='cup', token_tag='JJ', NER_tags='B-UNIT'),\n",
       " Pandas(sent_id=5, token_text='white', token_lemma='white', token_tag='JJ', NER_tags='B-NAME'),\n",
       " Pandas(sent_id=5, token_text='sugar', token_lemma='sugar', token_tag='NN', NER_tags='I-NAME')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_filtered[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min = np.argmax([len(x) for x in all_sents_filtered])\n",
    "max_len = len(all_sents_filtered[index_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = list(set([x.NER_tags for xs in all_sents_filtered for x in xs]))\n",
    "words = list(set([x.token_text for xs in all_sents_filtered for x in xs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-STATE',\n",
       " 'B-NAME',\n",
       " 'B-QUANTITY',\n",
       " 'SIZE',\n",
       " 'I-NAME',\n",
       " 'I-STATE',\n",
       " 'I-QUANTITY',\n",
       " 'I-UNIT',\n",
       " 'B-UNIT',\n",
       " 'TEMP',\n",
       " 'DF',\n",
       " 'O']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {word: idx + 2 for idx, word in enumerate(words)}\n",
    "\n",
    "word2index[\"UNKNOWN\"] = 0\n",
    "\n",
    "word2index[\"PADDING\"] = 1\n",
    "\n",
    "index2word = {idx: word for word, idx in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNKNOWN 0\n",
      "PADDING 1\n",
      "nan 2\n",
      "hazelnuts 3\n",
      "uncooked 4\n",
      "knead 5\n",
      "Spinach 6\n",
      "blanched 7\n",
      "double-acting 8\n",
      "hands 9\n"
     ]
    }
   ],
   "source": [
    "for k,v in sorted(word2index.items(), key=operator.itemgetter(1))[:10]:\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of the word uncooked is 4.\n",
      "The word with index 4 is uncooked.\n"
     ]
    }
   ],
   "source": [
    "test_word = \"uncooked\"\n",
    "\n",
    "test_word_idx = word2index[test_word]\n",
    "test_word_lookup = index2word[test_word_idx]\n",
    "\n",
    "print(\"The index of the word {} is {}.\".format(test_word, test_word_idx))\n",
    "print(\"The word with index {} is {}.\".format(test_word_idx, test_word_lookup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2index = {tag: idx + 1 for idx, tag in enumerate(tags)}\n",
    "\n",
    "tag2index[\"PADDING\"] = 0\n",
    "\n",
    "index2tag = {idx: word for word, idx in tag2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-STATE': 1,\n",
       " 'B-NAME': 2,\n",
       " 'B-QUANTITY': 3,\n",
       " 'SIZE': 4,\n",
       " 'I-NAME': 5,\n",
       " 'I-STATE': 6,\n",
       " 'I-QUANTITY': 7,\n",
       " 'I-UNIT': 8,\n",
       " 'B-UNIT': 9,\n",
       " 'TEMP': 10,\n",
       " 'DF': 11,\n",
       " 'O': 12,\n",
       " 'PADDING': 0}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "jalapeno\n",
      "peppers\n",
      ",\n",
      "seeded\n",
      "and\n",
      "minced\n"
     ]
    }
   ],
   "source": [
    "for x in x_train[4]:\n",
    "    print(index2word[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 5, 12, 1, 12, 6]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tag2index[tag] for tag in y_train_base[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[0]: ['2', 'jalapeno', 'peppers', ',', 'seeded', 'and', 'minced']\n",
      "y[0]: ['B-QUANTITY', 'B-NAME', 'I-NAME', 'O', 'B-STATE', 'O', 'I-STATE']\n",
      "X[0]: [1632, 1147, 2125, 1137, 619, 1079, 1641]\n",
      "y[0]: [3, 2, 5, 12, 1, 12, 6]\n",
      "X[0]: [1632, 1147, 2125, 1137, 619, 1079, 1641, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "y[0]: [3, 2, 5, 12, 1, 12, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "X[0]: [1632, 1147, 2125, 1137, 619, 1079, 1641, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "y[0]: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x_train_base = [[word.token_text for word in sent] for sent in all_sents_filtered]\n",
    "y_train_base = [[word.NER_tags for word in sent] for sent in all_sents_filtered]\n",
    "\n",
    "print(\"X[0]:\", x_train_base[4])\n",
    "print(\"y[0]:\", y_train_base[4])\n",
    "\n",
    "x_train = [[word2index[word] for word in sent] for sent in x_train_base]\n",
    "y_train = [[tag2index[tag] for tag in sent] for sent in y_train_base]\n",
    "\n",
    "print(\"X[0]:\", x_train[4])\n",
    "print(\"y[0]:\", y_train[4])\n",
    "\n",
    "x_train_padded = [sent + [word2index[\"PADDING\"]] * (max_len - len(sent)) for sent in x_train]\n",
    "y_train_padded = [sent + [tag2index[\"PADDING\"]] * (max_len - len(sent)) for sent in y_train]\n",
    "\n",
    "print(\"X[0]:\", x_train_padded[4])\n",
    "print(\"y[0]:\", y_train_padded[4])\n",
    "\n",
    "TAG_COUNT = len(tag2index)\n",
    "y_train_one_hot = [ np.eye(TAG_COUNT)[sent] for sent in y_train_padded]\n",
    "\n",
    "print(\"X[0]:\", x_train_padded[4])\n",
    "print(\"y[0]:\", y_train_one_hot[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[0]: [1632, 1147, 2125, 1137, 619, 1079, 1641, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "y[0]: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"X[0]:\", x_train_padded[4])\n",
    "print(\"y[0]:\", y_train_one_hot[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the training dataset: 7036\n",
      "Number of sentences in the test dataset : 1760\n"
     ]
    }
   ],
   "source": [
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(x_train_padded, y_train_one_hot, test_size=0.2, random_state=1234)\n",
    "\n",
    "print(\"Number of sentences in the training dataset: {}\".format(len(X_train_final)))\n",
    "print(\"Number of sentences in the test dataset : {}\".format(len(X_test_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_final[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_COUNT = len(index2word)\n",
    "DENSE_EMBEDDING = 50\n",
    "LSTM_UNITS = 50\n",
    "LSTM_DROPOUT = 0.1\n",
    "DENSE_UNITS = 100\n",
    "BATCH_SIZE = 256\n",
    "MAX_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_26 (InputLayer)       [(None, 25)]              0         \n",
      "                                                                 \n",
      " embedding_layer (Embedding  (None, 25, 300)           678300    \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional_27 (Bidirect  (None, 25, 100)           140400    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_28 (Bidirect  (None, 25, 200)           160800    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " bidirectional_29 (Bidirect  (None, 25, 100)           100400    \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " time_distributed_19 (TimeD  (None, 25, 25)            2525      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " crf (CRF)                   [(None, 25),              533       \n",
      "                              (None, 25, 13),                    \n",
      "                              (None,),                           \n",
      "                              (13, 13)]                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1082958 (4.13 MB)\n",
      "Trainable params: 1082958 (4.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow_addons.utils.types import FloatTensorLike, TensorLike\n",
    "from tensorflow_addons.layers import CRF\n",
    "from tensorflow_addons.losses import SigmoidFocalCrossEntropy\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "\n",
    "def build_model(max_len = max_len, input_dim = len(word2index),embedding_dim = 300):\n",
    "  # Model definition\n",
    "  input = Input(shape=(max_len,))\n",
    "\n",
    "  # Get embeddings\n",
    "  embeddings = Embedding(\n",
    "      input_dim,\n",
    "      embedding_dim,\n",
    "      # weights=[embedding_matrix],\n",
    "    input_length=max_len,\n",
    "       mask_zero=True,\n",
    "    trainable=True,\n",
    "      name = 'embedding_layer'\n",
    "  )(input)\n",
    "\n",
    "  # variational biLSTM\n",
    "  output_sequences = Bidirectional(LSTM(units=50, return_sequences=True))(embeddings)\n",
    "  output_sequences = Bidirectional(LSTM(units=100, return_sequences=True))(output_sequences)\n",
    "  # Stacking\n",
    "  output_sequences = Bidirectional(LSTM(units=50, return_sequences=True))(output_sequences)\n",
    "\n",
    "  # Adding more non-linearity\n",
    "  dense_out = TimeDistributed(Dense(25, activation=\"relu\"))(output_sequences)\n",
    "\n",
    "  # CRF layer\n",
    "  mask = Input(shape=(max_len,), dtype=tf.bool)\n",
    "  crf = CRF(13, name='crf')\n",
    "  predicted_sequence, potentials, sequence_length, crf_kernel = crf(dense_out)\n",
    "\n",
    "  model = Model(input, potentials)\n",
    "  model.compile(\n",
    "      optimizer=AdamW(weight_decay=0.001),\n",
    "      loss= SigmoidFocalCrossEntropy()) # Sigmoid focal cross entropy loss\n",
    "\n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# Checkpointing\n",
    "save_model = tf.keras.callbacks.ModelCheckpoint(filepath='ner_crf.h5',\n",
    "  monitor='val_loss',\n",
    "  save_weights_only=True,\n",
    "  save_best_only=True,\n",
    "  verbose=1\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, patience=10)\n",
    "\n",
    "callbacks = [save_model, es]\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = np.array(X_train_final)\n",
    "X_test_final = np.array(X_test_final)\n",
    "y_train_final = np.array(y_train_final)\n",
    "y_test_final = np.array(y_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25,)\n",
      "(25, 13)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].shape)\n",
    "print(y_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['chain_kernel:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - ETA: 0s - loss: 0.1862\n",
      "Epoch 1: val_loss improved from inf to 0.06156, saving model to ner_crf.h5\n",
      "110/110 [==============================] - 114s 739ms/step - loss: 0.1862 - val_loss: 0.0616\n",
      "Epoch 2/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0545\n",
      "Epoch 2: val_loss improved from 0.06156 to 0.04671, saving model to ner_crf.h5\n",
      "110/110 [==============================] - 63s 570ms/step - loss: 0.0545 - val_loss: 0.0467\n",
      "Epoch 3/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.1254\n",
      "Epoch 3: val_loss did not improve from 0.04671\n",
      "110/110 [==============================] - 66s 596ms/step - loss: 0.1254 - val_loss: 0.0792\n",
      "Epoch 4/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0667\n",
      "Epoch 4: val_loss did not improve from 0.04671\n",
      "110/110 [==============================] - 62s 566ms/step - loss: 0.0667 - val_loss: 0.0590\n",
      "Epoch 5/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0559\n",
      "Epoch 5: val_loss did not improve from 0.04671\n",
      "110/110 [==============================] - 62s 568ms/step - loss: 0.0559 - val_loss: 0.0540\n",
      "Epoch 6/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0515\n",
      "Epoch 6: val_loss did not improve from 0.04671\n",
      "110/110 [==============================] - 62s 567ms/step - loss: 0.0515 - val_loss: 0.0507\n",
      "Epoch 7/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0485\n",
      "Epoch 7: val_loss did not improve from 0.04671\n",
      "110/110 [==============================] - 63s 571ms/step - loss: 0.0485 - val_loss: 0.0479\n",
      "Epoch 8/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0466\n",
      "Epoch 8: val_loss improved from 0.04671 to 0.04585, saving model to ner_crf.h5\n",
      "110/110 [==============================] - 62s 565ms/step - loss: 0.0466 - val_loss: 0.0459\n",
      "Epoch 9/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0442\n",
      "Epoch 9: val_loss improved from 0.04585 to 0.04402, saving model to ner_crf.h5\n",
      "110/110 [==============================] - 61s 555ms/step - loss: 0.0442 - val_loss: 0.0440\n",
      "Epoch 10/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0422\n",
      "Epoch 10: val_loss improved from 0.04402 to 0.04184, saving model to ner_crf.h5\n",
      "110/110 [==============================] - 61s 557ms/step - loss: 0.0422 - val_loss: 0.0418\n",
      "Epoch 11/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0416\n",
      "Epoch 11: val_loss improved from 0.04184 to 0.04050, saving model to ner_crf.h5\n",
      "110/110 [==============================] - 63s 573ms/step - loss: 0.0416 - val_loss: 0.0405\n",
      "Epoch 12/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0393\n",
      "Epoch 12: val_loss improved from 0.04050 to 0.03878, saving model to ner_crf.h5\n",
      "110/110 [==============================] - 63s 569ms/step - loss: 0.0393 - val_loss: 0.0388\n",
      "Epoch 13/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0386\n",
      "Epoch 13: val_loss did not improve from 0.03878\n",
      "110/110 [==============================] - 63s 569ms/step - loss: 0.0386 - val_loss: 0.0391\n",
      "Epoch 14/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0380\n",
      "Epoch 14: val_loss improved from 0.03878 to 0.03735, saving model to ner_crf.h5\n",
      "110/110 [==============================] - 63s 573ms/step - loss: 0.0380 - val_loss: 0.0373\n",
      "Epoch 15/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0368\n",
      "Epoch 15: val_loss did not improve from 0.03735\n",
      "110/110 [==============================] - 66s 602ms/step - loss: 0.0368 - val_loss: 0.0396\n",
      "Epoch 16/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0364\n",
      "Epoch 16: val_loss improved from 0.03735 to 0.03676, saving model to ner_crf.h5\n",
      "110/110 [==============================] - 65s 590ms/step - loss: 0.0364 - val_loss: 0.0368\n",
      "Epoch 17/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0362\n",
      "Epoch 17: val_loss did not improve from 0.03676\n",
      "110/110 [==============================] - 65s 595ms/step - loss: 0.0362 - val_loss: 0.0403\n",
      "Epoch 18/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0369\n",
      "Epoch 18: val_loss improved from 0.03676 to 0.03519, saving model to ner_crf.h5\n",
      "110/110 [==============================] - 62s 567ms/step - loss: 0.0369 - val_loss: 0.0352\n",
      "Epoch 19/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0351\n",
      "Epoch 19: val_loss improved from 0.03519 to 0.03502, saving model to ner_crf.h5\n",
      "110/110 [==============================] - 61s 557ms/step - loss: 0.0351 - val_loss: 0.0350\n",
      "Epoch 20/20\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.0347\n",
      "Epoch 20: val_loss improved from 0.03502 to 0.03432, saving model to ner_crf.h5\n",
      "110/110 [==============================] - 61s 553ms/step - loss: 0.0347 - val_loss: 0.0343\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  X_train_final,\n",
    "  y_train_final,\n",
    "  batch_size=64,\n",
    "  epochs=20,\n",
    "  validation_data=(X_test_final, y_test_final),\n",
    "  callbacks=callbacks,\n",
    "  shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 30s 137ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tags = np.argmax(predictions, axis=-1)\n",
    "predicted_tags = [[index2tag[tag] for tag in sample] for sample in predicted_tags]\n",
    "\n",
    "flatten_predicted = [tag for sample in predicted_tags for tag in sample]\n",
    "flatten_true = [index2tag[np.argmax(tag)] for sample in y_train_final for tag in sample]\n",
    "\n",
    "unique_tags = list(set(flatten_true + flatten_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'PADDING': 141134,\n",
       "         'B-NAME': 8983,\n",
       "         'O': 8590,\n",
       "         'B-QUANTITY': 7036,\n",
       "         'B-UNIT': 6774,\n",
       "         'I-NAME': 3382,\n",
       "         'I-QUANTITY': 1})"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(flatten_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda4\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin\\miniconda4\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     B-STATE       0.00      0.00      0.00         0\n",
      "      B-NAME       0.71      0.54      0.61      8983\n",
      "  B-QUANTITY       0.99      0.94      0.97      7036\n",
      "        SIZE       0.00      0.00      0.00         0\n",
      "      I-NAME       0.45      0.63      0.52      3382\n",
      "     I-STATE       0.00      0.00      0.00         0\n",
      "  I-QUANTITY       0.02      1.00      0.04         1\n",
      "     PADDING       1.00      1.00      1.00    141134\n",
      "           O       0.75      0.58      0.65      8590\n",
      "      I-UNIT       0.00      0.00      0.00         0\n",
      "        TEMP       0.00      0.00      0.00         0\n",
      "          DF       0.00      0.00      0.00         0\n",
      "      B-UNIT       0.95      0.76      0.85      6774\n",
      "\n",
      "    accuracy                           0.94    175900\n",
      "   macro avg       0.37      0.42      0.36    175900\n",
      "weighted avg       0.96      0.94      0.95    175900\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda4\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\\n \", classification_report(flatten_predicted, flatten_true, labels=list(set(flatten_true))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-QUANTITY',\n",
       " 'B-UNIT',\n",
       " 'B-NAME',\n",
       " 'O',\n",
       " 'B-STATE',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING',\n",
       " 'PADDING']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
